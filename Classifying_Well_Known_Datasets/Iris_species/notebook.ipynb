{"metadata":{"_change_revision":0,"_is_fork":false,"language_info":{"name":"python","version":"3.6.1","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_cell_guid":"6e8b7ef8-79cd-3132-a050-03d1fd9dbda6","_uuid":"20fd5857df0f3d015974dcb90f4ad7656a4c0b60"},"execution_count":null,"outputs":[],"source":"I'm new to Machine Learning, therefore any inputs or suggestions would be very helpful for me.","cell_type":"markdown"},{"metadata":{"_cell_guid":"496469ec-d3aa-0b5f-df1c-992283364cc0","_uuid":"2b540634b946c03ceba40f37c57561fdaeacb94a","_execution_state":"idle","trusted":false},"execution_count":37,"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","cell_type":"code"},{"metadata":{"_cell_guid":"f61089a3-426f-a031-d698-c7a0b00d9671","_uuid":"c488830f7131c3582699455bbcbfe8d00282bcdd","_execution_state":"idle","trusted":false},"execution_count":38,"outputs":[],"source":"iris = pd.read_csv('../input/Iris.csv')\n\niris.head()# have a look at the dataset itself","cell_type":"code"},{"metadata":{"_cell_guid":"154c1511-0060-849f-7efa-3b83fb0c69b3","_uuid":"c913d2209aaedd75f72781118abc893c5b4b71b0","_execution_state":"idle","trusted":false},"execution_count":39,"outputs":[],"source":"#AT FIRST WE WILL DO SOME VISUALISTAION OF THE DATASET ITSELF\n# The first way we can plot things is using the .plot extension from Pandas dataframes\n# We'll use this to make a scatterplot of the Iris features.\niris.plot(kind=\"scatter\", x=\"SepalLengthCm\", y=\"SepalWidthCm\")","cell_type":"code"},{"metadata":{"_cell_guid":"fe593871-b2d0-3f20-bd61-09847146edb8","_uuid":"9b6308f37e1950830df2b5ff6791d5d58f779f37","_execution_state":"idle","trusted":false},"execution_count":null,"outputs":[],"source":"# We'll use seaborn's FacetGrid to color the scatterplot by species\nsns.FacetGrid(iris, hue=\"Species\", size=5) \\\n   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n   .add_legend()","cell_type":"code"},{"metadata":{"_cell_guid":"73b675a9-5663-a91e-d326-f420c9183146","_uuid":"46493a301d635b2bc44d612043b7084b9032646b","_execution_state":"busy","trusted":false},"execution_count":null,"outputs":[],"source":"# Another useful seaborn plot is the pairplot, which shows the bivariate relation\n# between each pair of features\n# From the pairplot, we'll see that the Iris-setosa species is separataed from the other\n# two across all feature combinations\nsns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=2)","cell_type":"code"},{"metadata":{"_cell_guid":"333be199-ef20-d258-770d-2ccc664060b1","_uuid":"179fa3e6899e6ba9b15d9f1ca9333b96b362bb61","_execution_state":"idle","trusted":false},"execution_count":32,"outputs":[],"source":"#We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n# shape\nprint(\"Shape is \" , iris.shape)\n#Statistical Summary\nprint(iris.describe())\n\n# class distribution\niris[\"Species\"].value_counts()\n","cell_type":"code"},{"metadata":{"_cell_guid":"54f3da65-ff60-8943-9826-8f3ad350c74e","_uuid":"7eacf37b62d87956abdd8af9a89d302aab61205c","_execution_state":"idle","trusted":false},"execution_count":33,"outputs":[],"source":"# Split-out validation dataset\narray = iris.values\nX = array[:,1:4]\nY = array[:,5]\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n\n#You now have training data in the X_train and Y_train for preparing models and a X_validation and Y_validation sets that we can use later.","cell_type":"code"},{"metadata":{"_cell_guid":"c96464e8-aa47-a934-fb13-2a452d54fa11","_uuid":"fcefac705a3891b03f27ab23c7fe7684a748c9c7","_execution_state":"idle","trusted":false},"execution_count":34,"outputs":[],"source":"'''\nLetâ€™s evaluate 6 different algorithms:\nLogistic Regression (LR)\nLinear Discriminant Analysis (LDA)\nK-Nearest Neighbors (KNN).\nClassification and Regression Trees (CART).\nGaussian Naive Bayes (NB).\nSupport Vector Machines (SVM).\n'''\n# Spot Check Algorithms\nseed = 7\nscoring = 'accuracy'\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name,model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n\tcv_results = model_selection.cross_val_score(model, X_train, Y_train , cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)       \n        ","cell_type":"code"},{"metadata":{"_cell_guid":"14d64d3f-9d9e-7c69-58ca-da0a237937a3","_uuid":"7baf7b655b423570907f9b4b9b3d6ecee763985a","_execution_state":"idle","trusted":false},"execution_count":35,"outputs":[],"source":"#We can see that it looks like KNN has the largest estimated accuracy score.\n#We can also create a plot of the model evaluation results and compare the spread and the mean accuracy of each model.\n# Compare Algorithms\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)","cell_type":"code"},{"metadata":{"_cell_guid":"f34dc51c-be6a-419b-588c-90465cb2c30d","_uuid":"27bc86baebf1c6e687ff5164bd9ac47b058d8902","_execution_state":"idle","trusted":false},"execution_count":36,"outputs":[],"source":"#Make Predictions\n#The KNN algorithm was one of the most accurate model that we tested. \n#Now we want to get an idea of the accuracy of the model on our validation set.\n# Make predictions on validation dataset\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\npredictions = knn.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n'''\nWe can see that the accuracy is 0.866667 or ~87%. \nThe confusion matrix provides an indication of the errors made. \nFinally, the classification report provides a breakdown of each class by precision, recall, f1-score and support showing excellent results (granted the validation dataset was small).\n'''\n\n","cell_type":"code"}]}
